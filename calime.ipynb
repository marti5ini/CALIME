{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T16:58:28.255728Z",
     "start_time": "2023-09-17T16:58:28.219881Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T21:06:52.886320Z",
     "start_time": "2023-09-17T21:06:52.009568Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lime'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/5z/b36mm7y11vlc1qc28_2y7_bw0000gn/T/ipykernel_22569/2379471426.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel_selection\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtrain_test_split\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 10\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0msrc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcalime_explainer\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mCALimeExplainer\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     11\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0msrc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcausal_model\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mget_causal_model\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0msrc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlime_explainer\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mLimeExplainer\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/PhD/Calime/CALIME/src/calime_explainer.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mnetworkx\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mnx\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mscipy\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mlime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlime_tabular\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mscipy\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mstats\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'lime'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.calime_explainer import CALimeExplainer\n",
    "from src.causal_model import get_causal_model\n",
    "from src.lime_explainer import LimeExplainer\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "import networkx as nx\n",
    "import pickle\n",
    "\n",
    "dataset_name = 'banknote'\n",
    "\n",
    "path = os.path.join(os.path.dirname(__file__), 'experiments', dataset_name)\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "dataset_path = os.path.join(os.path.dirname(__file__), 'data', dataset_name + '.csv')\n",
    "\n",
    "print(f\"Loading the {dataset_name} dataset...\")\n",
    "df = pd.read_csv(dataset_path, index_col=False)\n",
    "print(\"Dataset loaded successfully.\")\n",
    "print(\"Generating the causal model...\")\n",
    "\n",
    "# Split the DataFrame into features (X) and target labels (y).\n",
    "X, y = df.iloc[:, :-1], df.iloc[:, -1]\n",
    "\n",
    "# Create lists to store feature names and unique class values.\n",
    "feature_names, class_values = list(X.columns), list(y.unique())\n",
    "\n",
    "# Split the data into training and testing sets.\n",
    "num_samples = 100\n",
    "train, test = X.head(len(X) - num_samples), X.tail(num_samples)\n",
    "labels_train, labels_test = y.head(len(y) - num_samples), y.tail(num_samples)\n",
    "\n",
    "# Further split the training set into two subsets: 'train_bb' and 'train_gen'.\n",
    "# This is typically done for causal modeling purposes.\n",
    "train_bb, train_gen, y_bb, y_gen = train_test_split(train, labels_train, test_size=0.3, random_state=0)\n",
    "\n",
    "# Generate a causal model ('generative_model') and a graph representation ('graph')\n",
    "# using the 'train_gen' dataset, feature names, and a specified 'path'.\n",
    "graph_path = os.path.join(path, 'ground_truth.gpickle')\n",
    "generation_path = os.path.join(path, 'generative_model.pkl')\n",
    "if os.path.exists(graph_path) and os.path.exists(generation_path):\n",
    "    graph = nx.read_gpickle(graph_path)\n",
    "    with open(generation_path, 'rb') as file:\n",
    "        generative_model = pickle.load(file)\n",
    "else:\n",
    "    generative_model, graph = get_causal_model(train_gen, feature_names, path)\n",
    "print(\"Causal model and graph generated.\")\n",
    "\n",
    "print(\"Fitting the Black Box...\")\n",
    "# Create a RandomForestClassifier instance as the base estimator.\n",
    "estimator = RandomForestClassifier()\n",
    "hyper_param = dict(n_estimators=[100], #, 300, 500, 1000],\n",
    "                   max_depth=[5, 8, 15],\n",
    "                   min_samples_split=[2], #, 5, 10, 15, 100],\n",
    "                   min_samples_leaf=[1]#, 2, 5, 10]\n",
    "                   )\n",
    "# Perform hyperparameter tuning using RandomizedSearchCV with the specified hyperparameters.\n",
    "search = RandomizedSearchCV(estimator, hyper_param, verbose=1)\n",
    "\n",
    "search.fit(train_bb, y_bb)\n",
    "\n",
    "# Get the best estimator (model) from the hyperparameter tuning.\n",
    "best_estimator = search.best_estimator_\n",
    "\n",
    "# Use the trained 'best_estimator' model to make predictions on the test dataset.\n",
    "y_pred = best_estimator.predict(test)\n",
    "\n",
    "# Print a classification report that includes metrics like precision, recall, and F1-score,\n",
    "# comparing the predicted labels ('y_pred') to the true labels ('labels_test'),\n",
    "# considering the possible class values ('class_values').\n",
    "# print(\"Classification Report:\")\n",
    "# print(classification_report(labels_test, y_pred, labels=class_values))\n",
    "\n",
    "# Reset the index of the 'test' DataFrame.\n",
    "test = test.reset_index()\n",
    "\n",
    "# Select a random row from the 'test' DataFrame.\n",
    "random_row = test.sample(n=1)\n",
    "\n",
    "# Extract the feature values from the selected row ('data_row').\n",
    "data_row = random_row.iloc[:, 1:].values.flatten()\n",
    "\n",
    "# Print the index of the selected row from the 'test' DataFrame.\n",
    "print('Selected Row Index:', random_row.index[0])\n",
    "\n",
    "# LIME explanation\n",
    "lime_explainer = LimeExplainer(train_bb.values, feature_names=feature_names,\n",
    "                               class_names=class_values, discretize_continuous=False)\n",
    "\n",
    "lime_exp, lime_data, lime_neighbor_gen_time = lime_explainer.explain_instance(data_row, best_estimator.predict_proba)\n",
    "\n",
    "# CALIME explanation\n",
    "calime_explainer = CALimeExplainer(graph, generative_model, train_bb.values, feature_names=feature_names,\n",
    "                                   class_names=class_values, discretize_continuous=False)\n",
    "\n",
    "calime_exp, calime_data, calime_neighbor_gen_time = calime_explainer.explain_instance(data_row,\n",
    "                                                                                      best_estimator.predict_proba)\n",
    "\n",
    "lime_exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T14:36:38.258178Z",
     "start_time": "2023-09-17T14:36:38.181036Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_path = './data/banknote.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T14:36:54.669289Z",
     "start_time": "2023-09-17T14:36:54.578930Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/5z/b36mm7y11vlc1qc28_2y7_bw0000gn/T/ipykernel_19674/868763976.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mindex_col\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(dataset_path, index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "features_names = list(X.columns)\n",
    "class_values = list(y.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test set\n",
    "train, test = X.head(len(X) - num_samples), X.tail(num_samples)\n",
    "# save test \n",
    "#test.to_csv(path + '/test.csv', index_label='index')\n",
    "labels_train, labels_test = y.head(len(y) - num_samples), y.tail(num_samples)\n",
    "\n",
    "train_bb, train_gen, y_bb, y_gen = train_test_split(train, labels_train, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './experiments'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_model, graph = get_causal_model(train_gen, features_names, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = RandomForestClassifier()\n",
    "hyper_param = dict(n_estimators=[100, 300, 500, 1000],\n",
    "                        max_depth=[5, 8, 15],\n",
    "                        min_samples_split=[2, 5, 10, 15, 100],\n",
    "                        min_samples_leaf=[1, 2, 5, 10]\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = RandomizedSearchCV(estimator, hyper_param)\n",
    "search.fit(train_bb, y_bb)\n",
    "bb = search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bb.predict(test)\n",
    "#filename = 'bb_model.sav'\n",
    "#pickle.dump(bb, open(path '/' + filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(labels_test, y_pred, labels=class_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = test.sample(n=1)\n",
    "data_row = row.iloc[:, 1:].values.flatten()\n",
    "    \n",
    "print('index:', test.iloc[row.index[0], 0])\n",
    "print('data_row: ', data_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_explainer = LimeTabularExplainer(train_bb.values, feature_names=features_names,\n",
    "                                      class_names=class_values, discretize_continuous=False)\n",
    "\n",
    "lime_exp, lime_data, lime_neighbor_gen_time = lime_explainer.explain_instance(data_row, bb.predict_proba, \n",
    "                                                                              num_samples=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CALIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calime_explainer = CALimeTabularExplainer(train_bb.values, feature_names=features_names,\n",
    "                                          class_names=class_values, discretize_continuous=False)\n",
    "\n",
    "calime_exp, calime_data, calime_neighbor_gen_time = calime_explainer.explain_instance(data_row,\n",
    "                                                                                     bb.predict_proba,\n",
    "                                                                                     graph, generative_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "import sklearn.ensemble\n",
    "import numpy as np\n",
    "import lime.lime_tabular\n",
    "from __future__ import print_function\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = sklearn.datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, labels_train, labels_test = sklearn.model_selection.train_test_split(iris.data, iris.target, train_size=0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = sklearn.ensemble.RandomForestClassifier(n_estimators=500)\n",
    "rf.fit(train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime.lime_tabular.LimeTabularExplainer(train_bb.values, feature_names=features_names, class_names=class_values\n",
    "                                                   , discretize_continuous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, test.shape[0])\n",
    "exp = explainer.explain_instance(test[i], rf.predict_proba, num_features=2, top_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[i].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bb.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AMD (og, new): #\n",
    "    \n",
    "    og = og.values.tolist()\n",
    "    new = new.values.tolist()\n",
    "    \n",
    "    min_distances = []\n",
    "    \n",
    "    for record in new:\n",
    "        distances = []\n",
    "        for og_record in og:\n",
    "            d = dist (record, og_record)\n",
    "            distances.append(d)\n",
    "\n",
    "        min_distances.append(min(distances))\n",
    "\n",
    "    return statistics.mean(min_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_genetic = pd.read_json(\"/Users/martina/Downloads/df_genetic.json\")\n",
    "df_fair = pd.read_json(\"/Users/martina/Downloads/df_fair.json\")\n",
    "df_adult = pd.read_json(\"/Users/martina/Downloads/df_adult.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'statistics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-13-f8448d507173>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mAMD\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf_adult\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdf_genetic\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-11-6f04ce544781>\u001B[0m in \u001B[0;36mAMD\u001B[0;34m(og, new)\u001B[0m\n\u001B[1;32m     14\u001B[0m         \u001B[0mmin_distances\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdistances\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 16\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mstatistics\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmin_distances\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'statistics' is not defined"
     ]
    }
   ],
   "source": [
    "AMD(df_adult, df_genetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
